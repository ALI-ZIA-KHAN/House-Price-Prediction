{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "House Price Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPIotowc8wsA",
        "outputId": "3eea3005-89a9-43ba-f9bf-b4b071052d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) =boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgYP4ivm9AWq",
        "outputId": "639745d7-f563-4a43-8cce-af58ceb2f780"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9vGSFl19HeR",
        "outputId": "6f406a13-2874-4750-f8ef-2672ff1d5550"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8vR6o1g92gp",
        "outputId": "23477b20-662f-4cb7-ec61-8051b2a3569b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.27224633, -0.48361547, -0.43576161, -0.25683275, -0.1652266 ,\n",
              "       -0.1764426 ,  0.81306188,  0.1166983 , -0.62624905, -0.59517003,\n",
              "        1.14850044,  0.44807713,  0.8252202 ])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The network might be able to automatically adapt to such heterogeneous\n",
        "# data, but it would definitely make learning more difficult. A widespread best practice\n",
        "# to deal with such data is to do feature-wise normalization: for each feature in the input\n",
        "# data (a column in the input data matrix), you subtract the mean of the feature and\n",
        "# divide by the standard deviation, so that the feature is centered around 0 and has a\n",
        "# unit standard deviation. This is easily done in Numpy"
      ],
      "metadata": {
        "id": "JWDEPe62-Ha-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "#test data ka kch bhi expose nhi krna that's why we used same mean\n",
        "test_data -= mean\n",
        "test_data /= std"
      ],
      "metadata": {
        "id": "MWDBd1eA-iF8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu',\n",
        "  input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "#input_shape=(train_data.shape[1] ->13 neurons  0 likhte to row uthaleta\n",
        "#akhri layer 1 ki coz egresssion"
      ],
      "metadata": {
        "id": "NtEJgxaN_M80"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#k-fold validation\n",
        "import numpy as np\n",
        "k=4\n",
        "#we made 4 partitions\n",
        "num_val_samples = len(train_data) // k   #402//4->100 k 4 hisse\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "#ye code uthalena h,esha bs k apni mrzi se rkhlwna\n",
        "#0-99 ,100 ka chunk bnya pehle wo validation baqi partial_train_data mn traing krwadi \n",
        "#agli iteration mn agle 100 ko uthaya for validation baqi training\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "  partial_train_data = np.concatenate(\n",
        "  [train_data[:i * num_val_samples],\n",
        "  train_data[(i + 1) * num_val_samples:]],\n",
        "  axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "  [train_targets[:i * num_val_samples],\n",
        "  train_targets[(i + 1) * num_val_samples:]],\n",
        "  axis=0)\n",
        "\n",
        "\n",
        "\n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "  epochs=num_epochs, batch_size=1, verbose=0)\n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
        "  all_scores.append(val_mae)"
      ],
      "metadata": {
        "id": "CooHU0xg_2ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38d3334f-1b85-4503-dc72-10fe9e983d3e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing fold # 0\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 7.8222 - mae: 1.9826\n",
            "processing fold # 1\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 12.3024 - mae: 2.5105\n",
            "processing fold # 2\n",
            "4/4 [==============================] - 0s 3ms/step - loss: 14.2544 - mae: 2.6394\n",
            "processing fold # 3\n",
            "4/4 [==============================] - 0s 2ms/step - loss: 13.8735 - mae: 2.6304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " all_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpfZqorxJyrF",
        "outputId": "4a121def-ba73-42a2-91b1-bedd3e2bfec5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9825575351715088, 2.510498285293579, 2.639432430267334, 2.6304450035095215]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " np.mean(all_scores) #yanin abs error 2.5 mtlb agr 1000 ka ghr tha to 2.5% dur price btai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD9zK_iWJ2l4",
        "outputId": "4108768c-2ed3-45d0-825f-42106c62ed7e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.440733313560486"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}